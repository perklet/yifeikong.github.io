<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Yifei Kong | 读  Web Scraping with Python</title>
    <link rel="shortcut icon" type="image/png" href="/favicon.png">
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">
    <link href="/feed/atom-all.xml" type="application/atom+xml" rel="alternate" title="Yifei Kong Full Atom Feed" />
    <link href="/feed/rss-all.xml" type="application/rss+xml" rel="alternate" title="Yifei Kong Full RSS Feed" />
    <link href="/feed/atom.xml" type="application/atom+xml" rel="alternate" title="Yifei Kong Atom Feed" />
    <link href="/feed/rss.xml" type="application/rss+xml" rel="alternate" title="Yifei Kong RSS Feed" />
    <link rel="stylesheet" href="/theme/css/main.css" type="text/css" />
    <link rel="stylesheet" href="/theme/css/pygments.css" type="text/css" />
    <meta name="generator" content="Pelican" />
    <meta name="description" content="" />
    <meta name="author" content="Yifei Kong" />

    <meta name="keywords" content="读书笔记" />
</head>
<body>
    <header>
        <nav>
            <ul>
                <li><a href="/">Home</a></li>
                <li><a href="https://github.com/">GitHub</a></li>
                <li><a href="/tags">Tags</a></li>
                <li><a href="/categories">Categories</a></li>
                <li><a href="/archives">Archives</a></li>
            </ul>
        </nav>
        <div class="header_box">
            <h1><a href="/">Yifei Kong</a></h1>
        </div>
    </header>
    <div id="wrapper">
        <div id="content">            <h4 class="date">May 29, 2017</h4>

            <article class="post">
                <h2 class="title">
                    <a href="/posts/du-web-scraping-with-python.html" rel="bookmark" title="Permanent Link to &quot;读  Web Scraping with Python&quot;">读  Web Scraping with Python</a>
                </h2>

                
                

                <h1>Chapter I Introduction</h1>
<h2>为什么要写爬虫？</h2>
<ol>
<li>每个网站都应该提供 API，然而这是不可能的</li>
<li>即使提供了 API，往往也会限速，不如自己找接口</li>
</ol>
<p>注意已知条件（robots.txt 和 sitemap.xml）</p>
<ol>
<li>robots.txt 中可能会有陷阱</li>
<li>sitemap 中可能提供了重要的链接</li>
</ol>
<h2>估算网站的大小</h2>
<p>一个简便方法是使用 site:example.com 查询，然而这种方法对于大站不适用</p>
<h2>识别网站所使用的技术</h2>
<ol>
<li>builtwith 模块</li>
</ol>
<div class="highlight"><pre><span></span>pip install builtwith
builtwith.parse(url) # returns a dict
</pre></div>


<ol>
<li>python-whois 模块</li>
</ol>
<div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">python</span><span class="o">-</span><span class="n">whois</span>
<span class="kn">import</span> <span class="nn">whois</span>
<span class="n">whois</span><span class="o">.</span><span class="n">whois</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
</pre></div>


<h2>下载器</h2>
<p>下载器需要提供的几个功能：</p>
<ol>
<li>错误重试，仅当返回的错误为500的时候重试，一般400错误可认为不可恢复的网页</li>
<li>伪装 UA</li>
<li>策略
    a. 爬取站点地图 sitemap
    b. 通过 ID 遍历爬取
        i. ID 可能不是连续的，比如某条记录被删除了
        ii. ID 访问失效 n 次以后可以认为遍历完全了</li>
<li>相对连接转化，这点可以利用 lxml 的 make_link_absolute 函数</li>
<li>处理 robots.txt 可以利用标准库的 robotsparser 模块</li>
</ol>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">robotsparser</span>
<span class="n">rp</span> <span class="o">=</span> <span class="n">robotparser</span><span class="o">.</span><span class="n">RobotFileParser</span>
<span class="n">rp</span><span class="o">.</span><span class="n">set_url</span><span class="p">(</span><span class="s1">&#39;path_to_robots.txt&#39;</span><span class="p">)</span>
<span class="n">rp</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">rp</span><span class="o">.</span><span class="n">can_fetch</span><span class="p">(</span><span class="s2">&quot;UA&quot;</span><span class="p">,</span> <span class="s2">&quot;url&quot;</span><span class="p">)</span>
<span class="bp">True</span> <span class="ow">or</span> <span class="bp">False</span>
</pre></div>


<ol>
<li>支持代理</li>
<li>下载限速，粒度应该精确到每一个站点比较好</li>
<li>避免爬虫陷阱，尤其是最后一页自身引用自身的例子
   a. 记录链接深度</li>
</ol>
<p>例子：https://bitbucket.org/wswp/code/src/chpter01/link_crawler3.py</p>
<h1>Chapter II Scraping</h1>
<h2>抽取资源的方式</h2>
<ol>
<li>正则
        不适用于匹配网页结构，因为网页结构中空白等都是无关紧要的，而可能破坏正则 Structural-based
        适用于数据本身符合某种模式，比如 IP 地址，比如日期等 Content-based</li>
<li>xpath 与 CSS
        适用于匹配网页的结构信息 Strctual-based，lxml 的 CSS 选择器在内部是转换为 xpath 实现的，css 远不如 xpath 灵活</li>
<li>BeautifulSoup，慢，从来没有在生产代码中见到过</li>
</ol>
<p>下载的第二步，就是把获得的网页传递给 Extractor 来提取内容，可以通过传递给下载函数回调来处理，但是这种耦合性太强了</p>
<h1>Chapter III Downloader Cache</h1>
<ul>
<li>书中的缓存把所有相应都做了缓存，包括500的错误响应，实际上这样的直接不缓存好了。。</li>
<li>书中的磁盘缓存把 url normalize 逻辑也加到了这里，感觉比较混乱</li>
<li>注意使用磁盘文件缓存的话会受限于磁盘单目录文件的数量，即使是 ext4 文件系统也不大</li>
</ul>
<h1>Chapter IV</h1>
<p>执行下载时间估算也是很重要的，每个链接下载需要多长时间，整个过程需要多长时间
多线程的下载例子，手工模拟线程池</p>
<div class="highlight"><pre><span></span>def process_queue(q):
    pass

threads = []
while thread or crawl_queue:
    for thread in threads:
        if not threads.is_alive():            
            threads.remove(thread)
    while len(threads) &lt; max_threads and crawl_queue:
        thread = threading.Thread(target=process_queue, daemon=True)
        thread.start()
        threads.append(thread)
    time.sleep(some_time)
</pre></div>


<p>性能的增长与线程和进程的数量并不是成线性比例的，而是对数比例，因为切换要花费一定的时间，再者最终是受限于带宽的</p>
<h1>Chapter V Dynamic Content</h1>
<h2>逆向接口</h2>
<p>依赖于 Ajax 的网站看起来更复杂，但是实际上因为数据和表现层的分离会更简单，但是如果逆向工程也不好得到通用的方法，如何构建一个辅助工具呢？表示出网页上哪些地方是动态加载的，列出 js 全局变量，列出可能的 jsonp 请求</p>
<p>利用 Ajax 接口时，可以利用各种边界情况，比如把搜索条件置为空，置为 *，置为 .</p>
<h2>渲染动态网页</h2>
<p>使用Qt，使用 Selenium 或者 PhantomJS，这时附加 Cookie 等都是很严重的问题</p>
<h1>Chapter VI Form Interaction</h1>
<ul>
<li>登录表单中往往会有隐藏的参数，比如 form_key 用于避免表单重复提交，还可能需要 cookie 验证</li>
<li>Wow，竟然可以直接从浏览器加载 Cookie，使用 browsercookie 模块</li>
</ul>
<h1>Chapter VII</h1>
<p>使用机器识别验证码
使用 Pillow 和 pytesseract 识别验证码，但是 tesseract 本不是用来识别验证码的</p>
<h2>一种锐化方法</h2>
<div class="highlight"><pre><span></span>img.convert(&#39;L&#39;)
img.point(lambda x: 0 if x &lt; 1 else 255, &#39;l&#39;)
tessact.image_to_string(img)
</pre></div>


<p>还可以通过限定字符集提高识别率</p>
<p>还可以使用人工打码平台</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="/posts/du-web-scraping-with-python.html">posted at 14:31</a>
                    &nbsp;&middot;&nbsp;<a href="/category/du-shu-bi-ji.html" rel="tag">读书笔记</a>
                    &nbsp;&middot;
                    &nbsp;<a href="/tag/du-shu-bi-ji.html" class="tags">读书笔记</a>
                </div>
            </article>
            <div class="clear"></div>
            <footer>
                <p>
                <a href="https://github.com/jody-frankowski/blue-penguin">Blue Penguin</a> Theme
                &middot;
                Powered by <a href="http://getpelican.com">Pelican</a>
                &middot;
                <a href="/feed/atom-all.xml" rel="alternate">Atom Feed</a>
                &middot;
                <a href="/feed/rss-all.xml" rel="alternate">Rss Feed</a>
            </footer>
        </div>
        <div class="clear"></div>
    </div>
</body>
</html>